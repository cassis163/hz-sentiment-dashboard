{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:80: E501 line too long (86 > 79 characters)\n"
     ]
    }
   ],
   "source": [
    "# OZ stands for Omroep Zeeland\n",
    "OZ_DISQUS_API_KEY = 'E8Uh5l5fHZ6gD8U3KycjAIAk46f68Zw7C6eW8WSjZvCLXebZ7p0r1yrYDrLilk2F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from json import loads\n",
    "\n",
    "\n",
    "def get_disqus_data(url, identifier):\n",
    "    '''\n",
    "    Retrieves a Disqus data on a web page.\n",
    "    '''\n",
    "    response = get(\n",
    "        'https://disqus.com/api/3.0/threads/details',\n",
    "        params={\n",
    "            'thread': 'ident:' + identifier,\n",
    "            'forum': 'omroepzeeland',\n",
    "            'api_key': OZ_DISQUS_API_KEY\n",
    "        }\n",
    "    )\n",
    "    return loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from re import findall\n",
    "\n",
    "\n",
    "def find_element(url, method, element, class_='', parser='html.parser'):\n",
    "    '''\n",
    "    Performs web scraping given the url, elements\n",
    "    and class to look for.\n",
    "    '''\n",
    "    # Get the HTML of the page\n",
    "    text = get(url).text\n",
    "    soup = BeautifulSoup(text, parser)\n",
    "    return getattr(soup, method)(\n",
    "        element,\n",
    "        class_=class_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape_omroepzeeland(url):\n",
    "    '''\n",
    "    Returns a list of all the comments on the web page.\n",
    "    Each comment is a dictionary.\n",
    "    '''\n",
    "    def get_identifier(script_string):\n",
    "        '''\n",
    "        Find all parts between parenthesis\n",
    "        and take the second one (the identifier)\n",
    "        '''\n",
    "        return findall(\"'(.*?)'\", script_string)[1]\n",
    "\n",
    "    def get_comment_data(feed_url, page_url):\n",
    "        def remove_tags(string, tags):\n",
    "            for tag in tags:\n",
    "                string = string.replace(tag, '')\n",
    "            return string\n",
    "\n",
    "        items = find_element(\n",
    "            feed_url, 'find_all', 'item'\n",
    "        )\n",
    "\n",
    "        # Check if there are any comments\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                'creator': item.find('dc:creator').text,\n",
    "                'text': remove_tags(\n",
    "                    item.find('description').text,\n",
    "                    [\n",
    "                        '<br>', '<br/>', '<br />',\n",
    "                        '<p>', '</p>'\n",
    "                    ]\n",
    "                ),\n",
    "                'link': page_url,\n",
    "                'timestamp': item.find('pubdate').text\n",
    "            }\n",
    "            for item in items\n",
    "        ]\n",
    "    script = find_element(url, 'find', 'div', class_='news-comments')\n",
    "    if script:\n",
    "        script = script.find('script').string\n",
    "    else:\n",
    "        # Page does not have a div with class news-comments\n",
    "        return\n",
    "\n",
    "    identifier = get_identifier(script)\n",
    "    disqus_data = get_disqus_data(url, identifier)\n",
    "\n",
    "    return get_comment_data(\n",
    "        disqus_data['response']['feed'],\n",
    "        url\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrape(sources_df):\n",
    "    '''\n",
    "    Returns a DataFrame with web scraped data\n",
    "    from the input DataFrame.\n",
    "    '''\n",
    "    comments_df = pd.DataFrame(\n",
    "        columns=['creator', 'text', 'link', 'timestamp']\n",
    "    )\n",
    "    for index, _ in sources_df.iterrows():\n",
    "        # Where index is an URL (link)\n",
    "        series_dict = web_scrape_omroepzeeland(index)\n",
    "        # Check if there are any comments\n",
    "        if series_dict:\n",
    "            comments_df = comments_df.append(\n",
    "                series_dict, ignore_index=True\n",
    "            )\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   creator                                               text  \\\n",
      "0  Bastiaan van Stapelberg  Dit is zulke verschrikkelijke onzin. Ik heb he...   \n",
      "1        disqus_qR1IbSLz73  Waarop gebaseerd? Niet op de mening van de stu...   \n",
      "2               HZ Student  Ik kan niet spreken voor alle opleidingen binn...   \n",
      "3          Kuchende meneer  Hoe zit het nu met HBO-ICT?, of mogen wij daar...   \n",
      "4               T'is roak!  Hier nog een anonieme insider. Deze sluit zich...   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.omroepzeeland.nl/nieuws/122990/HZ-...   \n",
      "1  https://www.omroepzeeland.nl/nieuws/122990/HZ-...   \n",
      "2  https://www.omroepzeeland.nl/nieuws/122990/HZ-...   \n",
      "3  https://www.omroepzeeland.nl/nieuws/108573/HZ-...   \n",
      "4  https://www.omroepzeeland.nl/nieuws/101070/HZ-...   \n",
      "\n",
      "                         timestamp  \n",
      "0  Thu, 08 Oct 2020 06:23:20 -0000  \n",
      "1  Thu, 08 Oct 2020 05:48:38 -0000  \n",
      "2  Thu, 08 Oct 2020 05:31:13 -0000  \n",
      "3  Thu, 04 Oct 2018 03:19:33 -0000  \n",
      "4  Tue, 26 Sep 2017 15:50:46 -0000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os import getcwd\n",
    "\n",
    "sources_df = pd.read_csv(getcwd() + '/data/sources.csv', index_col='link')\n",
    "comments_df = web_scrape(sources_df)\n",
    "\n",
    "print(comments_df.head())\n",
    "comments_df.to_csv(getcwd() + '/data/comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
