Woensdag 26 juni 2013, 19:02 Twitteraccount H2OKITS meldt op Twitter dat Nederland big data gaat gebruiken bij watermanagement.


De tweet verwijst via via naar een artikel met de titel The Netherlands looks to big data tot tackle floods, op de site van The Wall Street Journal. In het artikel wordt verteld dat Rijkswaterstaat, Waterschap Delfland, Deltares en de TU Delft van plan zijn om – in samenwerking met IBM – big data te gaan gebruiken om overstromingen te voorspellen, de response daarop te verbeteren en een informatieplatform te bieden aan ondernemers en wetenschappers.

Woensdag 26 juni 2013, 10:00 uur: in de Aletta Jacobszaal in Den Haag vindt een rondetafelgesprek plaats over praktijken, gevolgen en wettelijke kaders inzake het aftappen van persoonsgegevens, waarvoor verschillende deskundigen zijn uitgenodigd.

Zaterdag 22 juni 2013, NRC Handelsblad: twee uitgebreide verhalen over ‘big data’.


Dit zijn nog maar drie van de talloze gebeurtenissen en verhalen in kranten, op websites, forums en Twitter die te maken hebben met het onderwerp big data, en dan is Prism en het feit dat de NSA (National Security Agency) in het geheim persoonsgegevens ‘aftapt’ of opvraagt van bedrijven zoals Facebook, Apple en Google, nog niet eens genoemd.

Het is een vreemde gewaarwording om een boek te lezen dat voor je ogen wereldnieuws wordt, want in De big datarevolutie vertellen Viktor Mayer-Schönberger en Kenneth Cukier wat er met big data wordt bedoeld en waarom bedrijven en overheden, waaronder de NSA, zo graag zo veel gegevens van en over ons willen hebben. Maar wat is big data nu eigenlijk? Volgens de schrijvers is big data het vermogen van de samenleving om informatie op nieuwe manieren in te zetten voor het verkrijgen van nuttige inzichten of waardevolle goederen en diensten.

Een voorbeeld daarvan is Google als griepvoorspeller. Google kon in 2009 veel sneller, vrijwel real time zelfs, laten zien hoe griep zich verplaatste en verspreidde dan de centra voor ziektebestrijding en –preventie (CDC, centers for Disease Control en Prevention) dat konden. Dat kwam omdat Google gebruik maakte van de drie miljard (!) zoekopdrachten die het bedrijf dagelijks verwerkt en bewaart. Google bekeek de 50 miljoen zoektermen die het meest werden ingetypt en vergeleek die met de gegevens van de CDC’s over de verspreiding van de griep in de periode van 2003 tot en met 2008. Die vergelijking was nodig om te kunnen achterhalen wie geïnfecteerd was, op basis van wat mensen op dát moment op internet zochten. Dat was door anderen eerder ook al wel geprobeerd, maar ten eerste beschikten die anderen niet over zoveel gegevens en zoveel rekenkracht als Google, en ten tweede werd toen aangenomen dat zoektermen zoals ‘medicijn voor hoest en koorts’ daarbij belangrijk zouden zijn. Dat bleek een verkeerde aanname.
Wat Google vervolgens deed was de wereld op zijn kop zetten door niet te gaan zoeken met behulp van een verzameling zoektermen, maar de gegevens zelf te laten spreken: Google zocht naar correlaties tussen hoe vaak een bepaalde zoekterm voorkwam en de verspreiding van de griep in tijd en ruimte. Dat was overigens geen peulenschil, want ze verwerkten maar liefst 450 miljoen verschillende wiskundige modellen voor het testen van de zoekopdrachten. Het resultaat was echter wel dat Google, toen in 2009 het H1N1-virus toesloeg niet alleen sneller, maar ook bruikbaardere gegevens over de verspreiding van het virus kon opleveren dan de CDC’s, simpelweg omdat de meeste mensen eerst op internet gaan zoeken alvorens naar een arts te gaan. Als ze al naar een arts gaan. En Google leverde niet alleen real time informatie over de verspreiding van het virus, maar kon ook vrij precies het verloop van de epidemie voorspellen. Onschatbare informatie in het voorkomen van een pandemie, natuurlijk.


(bron: The graphic recorder)

Viktor Mayer-Schönberger en Kenneth Cukier geven in het boek nog veel meer voorbeelden van big data. En telkens maken ze duidelijk dat het gaat om heel veel data, om correlaties (en absoluut niet om causale verbanden), en om ‘slordige’ data. Met slordige data wordt bedoeld dat er geen pogingen worden ondernomen om, zoals dat bij steekproeven wel gebeurd, heel zorgvuldig te zijn in het samenstellen van een verzameling gegevens. Waarom het niet belangrijk is om zorgvuldig en van tevoren de gegevens te selecteren, maken ze duidelijk aan de hand van een foto.
Als je met een gewoon fototoestel een foto neemt, dan moet je vooraf bepalen welk deel van de foto scherp moet worden. Dat kun je vergelijken met nemen van een steekproef. Bij een steekproef bepaal je ook vooraf wat je te weten wilt komen en welke gegevens je nodig hebt om een goede steekproef te kunnen uitvoeren. Dat gebeurt niet bij big data. Juist niet. Big data gebruikt vaak gegevens die er al zijn. Gegevens die niet verzameld werden om déze vraag te beantwoorden, maar data die toch al verzameld werd om een andere reden. Denk aan de zoekopdrachten en de griepepidemie. Het is met big data net zoals bij een foto gemaakt met een camera van Lytro: daarmee kun je foto’s achteraf scherp stellen. Dat achteraf scherpstellen kan alleen maar als alles op de foto scherp wordt opgeslagen. Dat kost, vergeleken met een normale foto, wel veel meer opslag, maar er is opslagruimte genoeg tegenwoordig en het kost vrijwel niets meer. Het is mede daardoor dat big data pas sinds een aantal jaren echt interessant is en steeds interessanter wordt: de data is niet – zoals bij steekproeven – maar voor één keer te gebruiken, het kan diverse keren worden gebruikt door iedere keer een ander deel ‘scherp te stellen’. Daarom is het handig om zoveel als je kan op te slaan én dat allemaal te bewaren. De big datarevolutie bevat veel voorbeelden van het gebruik van big data op diverse terreinen, zoals gezondheid, vliegtuigtickets, voorkomen van in de lucht vliegen van putdeksels, miljoenen besparen op onderhoud van auto’s, patrouilleerroutes van politie, en nog veel en veel meer.

Is het alleen maar goed nieuws? Heeft big data geen nadelen? Nee, natuurlijk heeft big data nadelen. Zoals het griepvoorbeeld liet zien, kan big data worden ingezet om voorspellingen te doen. Big datavoorspellingen worden echter niet alleen gebruikt om te voorspellen hoe een griepepidemie zal gaan verlopen, of welke gebieden in Nederland wellicht onder water komen te staan, maar ook waar morgen waarschijnlijk misdrijven gaan worden gepleegd en door wie.


(bron: digitalust.wordpress.com)

In de BBC Horizon documentaire The age of big data (Engelstalig) is te zien hoe de politie in Los Angelos patrouilles uitvoert in wijken (en daar mensen aanhoudt) op basis van big data analyses. Ondanks scepsis in het begin, zijn de agenten nu enthousiast omdat sinds de invoering van het nieuwe systeem de criminaliteit vele percentages is gedaald …

Mayer-Schönberger en Cukier zijn echter de eersten om te waarschuwen voor het trekken van verkeerde conclusies, door te wijzen op het gevaar van oorzaak-en-gevolg-denken in dit soort situaties: dat de criminaliteit gedaald is, hoeft niet één op één het gevolg te zijn van het patrouilleren op basis van big datavoorspellingen. Er kunnen andere oorzaken zijn die niet meer ontdekt worden als daar niet meer naar wordt gezocht, omdat klakkeloos wordt aangenomen dat de nieuwe patrouilleeraanpak daarvoor verantwoordelijk is. Bovendien moet er niet uit het oog worden verloren dat – hoewel big data voorspellingen steeds preciezer en betrouwbaarder worden – de gebeurtenissen niet werkelijk hoeven plaats te vinden. Nu wordt er al gepatrouilleerd en, zoals in de BBC documentaire te zien is, worden mensen gefouilleerd op basis van big datavoorspellingen. Is de volgende stap dat je aangehouden en misschien zelfs gestraft kunt gaan worden op basis van diezelfde voorspellingen? Als dat gebeurt, zijn concepten als vrije wil, gelijke behandeling en rechtvaardigheid, niets meer waard.


(bron: news.discovery.com: Is Minority Report Here?)

De gevolgen zijn in de film Minority Report goed te zien. Daarin spelen drie ‘genetisch gemodificeerde mediums’ de rol die big data binnenkort kan hebben of nu al heeft in sommige gevallen. De mediums in Minority Report zijn in staat te voorspellen waar, wanneer en door wie een moord gaat worden gepleegd, zodat de potentiële moordenaar voor die tijd kan worden opgepakt. Dankzij deze voorspellers is zes jaar lang voorkomen dat er moorden zijn gepleegd, maar het probleem is natuurlijk dat er eigenlijk niet kan worden bewezen dat de gearresteerden ook echt schuldig zijn. Ze zijn tenslotte al vóór het plegen daarvan opgepak. Hoe kun je als gearresteerde aantonen dat je die moord niet gepleegd zou hebben? Ze zijn ‘schuldig, totdat ze hun onschuld kunnen bewijzen’, maar dat laatste zal geen van hen kunnen. En dus zijn ze schuldig en worden ze gevangen gezet.

Hoe precair deze situatie ook is, de mogelijkheid om op basis van veel gegevens en correlaties daartussen, te voorspellen wie, wanneer, wat gaat doen is de reden dat de NSA de gegevens van telefoonbedrijven, Facebook, Google en al die andere dataverzamelaars, wil hebben. Welke gevolgen big data straks, of eigenlijk nu al, heeft voor onze privacy of voor de rechtspraak is nog moeilijk te voorspellen. Hoe kun je bijvoorbeeld toestemming geven voor het gebruik van gegevens waarvan je niet weet waarvoor ze straks gebruikt zullen worden? Hoe kunnen bedrijven aan jou toestemming vragen voor het gebruik van jouw gegevens, terwijl ze nu nog niet weten waar ze die gegevens nog meer voor willen gaan gebruiken? Hoe voorkom je misbruik van gegevens en hoe voorkom of bestraf je slordige analyses, op basis waarvan verkeerde voorspellingen plaatsvinden? Wie helpt de slachtoffers van dat soort fouten? Dat moet allemaal nog worden uitgedacht en uitgewerkt. Maar dat op die gebieden een en ander gaat veranderen staat als een paal boven water. Net zoals het kristalhelder is dat er voor bedrijven en personen die zich met data analyse bezighouden gouden tijden aanbreken.

Mayer-Schönberger en Cukier hebben met De big datarevolutie een begrijpelijk, en aangenaam leesbaar boek geschreven dat niet alleen de donkere kant van big data laat zien, maar ook hoe we het kunnen gebruiken om op tal van terreinen vooruitgang te boeken, die op een andere manier niet gerealiseerd had kunnen worden. Of anders veel langer op zich zou hebben laten wachten. Het enige minpunt van het boek is dat de gebruikte indeling tot gevolg heeft dat bij diverse onderwerpen dezelfde voorbeelden worden gebruikt en die herhaling is niet prettig. Dat neemt echter niet weg dat iedereen dit boek eigenlijk zou moeten lezen, omdat het over ons, onze gegevens en onze toekomst gaat.

Gelezen in juni 2013